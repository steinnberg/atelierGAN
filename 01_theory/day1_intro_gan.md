# Jour 1 â€“ Introduction aux GANs (Generative Adversarial Networks)

## ğŸ¯ Objectifs du jour

- Comprendre le principe fondamental des GANs
- Ã‰tudier lâ€™architecture de base : GÃ©nÃ©rateur vs Discriminateur
- Explorer les fonctions de coÃ»t et l'entraÃ®nement par jeu Ã  somme nulle
- ImplÃ©menter un GAN simple pour gÃ©nÃ©rer des images

---

## ğŸ¤– Quâ€™est-ce quâ€™un GAN ?

Un **GAN** est un type de rÃ©seau de neurones gÃ©nÃ©ratif qui repose sur une **confrontation** entre deux modÃ¨les :
- **GÃ©nÃ©rateur (G)** : crÃ©e de nouvelles donnÃ©es Ã  partir dâ€™un bruit alÃ©atoire
- **Discriminateur (D)** : essaie de distinguer les vraies donnÃ©es des donnÃ©es gÃ©nÃ©rÃ©es

### ğŸ§  IdÃ©e clÃ© : Apprentissage par confrontation
Câ€™est un **jeu Ã  somme nulle** entre deux rÃ©seaux :
- Le gÃ©nÃ©rateur apprend Ã  "tromper" le discriminateur
- Le discriminateur apprend Ã  mieux dÃ©tecter les fausses donnÃ©es

> ğŸ§¾ RÃ©fÃ©rence fondatrice :
> [Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661)

---

## ğŸ—ï¸ Architecture GAN de base

```plaintext
Bruitage z ~ N(0,1)
    â”‚
    â–¼
[ GÃ©nÃ©rateur G ]
    â”‚
    â–¼
Image gÃ©nÃ©rÃ©e
    â–¼
[ Discriminateur D ] â† Image rÃ©elle
    â”‚
    â–¼
PrÃ©diction : vraie ou fausse ?

Le gÃ©nÃ©rateur prend un vecteur alÃ©atoire (bruit) comme entrÃ©e et produit une image.

Le discriminateur Ã©value si cette image est rÃ©elle ou gÃ©nÃ©rÃ©e.

```
# ğŸ§® Fonction de coÃ»t (minimax)

La fonction de coÃ»t des GANs est une fonction **minimax** :

\[
\min_G \max_D \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
\]

- Le **discriminateur maximise** sa capacitÃ© Ã  dÃ©tecter les vrais/faux
- Le **gÃ©nÃ©rateur minimise** la capacitÃ© du discriminateur Ã  le repÃ©rer

---

# ğŸ§¨ ProblÃ¨mes courants

| ProblÃ¨me            | Description                                       |
|---------------------|---------------------------------------------------|
| Mode Collapse       | Le gÃ©nÃ©rateur produit toujours les mÃªmes exemples |
| Training Instability| Oscillations ou convergence difficile             |
| Vanishing Gradients | D devient trop fort, G nâ€™apprend plus             |

---

# ğŸ› ï¸ EntraÃ®nement dâ€™un GAN simple (MNIST)

Aujourdâ€™hui, on implÃ©mente un **GAN simple sur MNIST** :

- **GÃ©nÃ©rateur** : quelques couches `Linear` + `ReLU`
- **Discriminateur** : `Linear` + `LeakyReLU`
- **Perte** : `BCELoss` (Binary Cross Entropy)

---

# ğŸ’¡ Concepts clÃ©s Ã  retenir

- **Nash Equilibrium** : point oÃ¹ G et D nâ€™ont plus dâ€™intÃ©rÃªt Ã  changer
- **Latent space** : espace de bruit structurÃ© permettant la gÃ©nÃ©ration
- **Adversarial loss** : moteur de lâ€™apprentissage

---

# ğŸ“š RÃ©fÃ©rences complÃ©mentaires

- [Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
- [GAN Hacks â€“ Soumith Chintala](https://github.com/soumith/ganhacks)
- [A Visual Introduction to GANs](https://poloclub.github.io/ganlab/)
- [Cours au CollÃ¨ge de France](https://www.college-de-france.fr/fr/agenda/cours/generation-de-donnees-en-ia-par-transport-et-debruitage/generation-de-donnees-en-ia-par-transport-et-debruitage-1)

---

# ğŸ” Prochaine Ã©tape

ğŸ‘‰ Passe au notebook [`02_labs/images_gan/train_gan_images.ipynb`](../02_labs/images_gan/train_gan_images.ipynb) pour coder ton premier GAN ğŸ¨


